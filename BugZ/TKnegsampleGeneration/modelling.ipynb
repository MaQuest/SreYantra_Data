{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Naive Bayes Classifaction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utilities as util\n",
    "import importlib\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score,train_test_split\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score,confusion_matrix,classification_report\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utilities' from 'C:\\\\Users\\\\teddy\\\\Documents\\\\Research Project\\\\SreYantra_Data\\\\BugZ\\\\TKnegsampleGeneration\\\\utilities.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(util)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establishing global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_pairs = \"Teddy_Data/AllDependentPairs.csv\"\n",
    "I_pairs = \"Teddy_Data/AllIndependentPairs.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dp = pd.read_csv(D_pairs, low_memory = False)\n",
    "df_ip = pd.read_csv(I_pairs, low_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnamed_columns = [\"Unnamed: 0\", \"Unnamed: 0.1\"]\n",
    "df_dp = df_dp.drop(columns = unnamed_columns)\n",
    "df_ip = df_ip.drop(columns = unnamed_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of independent pairs is 62011\n",
      "The amount of dependent pairs is 740230\n"
     ]
    }
   ],
   "source": [
    "print(\"The amount of independent pairs is {}\".format(len(df_dp)))\n",
    "print(\"The amount of dependent pairs is {}\".format(len(df_ip)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model for future uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "count_vect = util.create_vectorizor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set 80-20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 1000\n",
    "split = int(total/2)\n",
    "\n",
    "train = int(0.8*total)\n",
    "test = int(0.2*total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_dp[df_dp[\"req1Product\"] == \"Firefox\"].head(split)\n",
    "df = df.append(df_ip[df_ip[\"req1Product\"] == \"Firefox\"].head(split))\n",
    "# randomize the dataframe\n",
    "df = df.sample(frac = 1)\n",
    "# get the test series'\n",
    "binary_class = df[\"BinaryClass\"]\n",
    "multi_class = df[\"MultiClass\"]\n",
    "## drop unimportant columns from training\n",
    "sub_df = df.drop(columns = ['BinaryClass', 'MultiClass',\"req1Product\",\"req2Product\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split up the train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = sub_df.head(train)\n",
    "train_y = binary_class.head(train)\n",
    "test_x = sub_df.tail(test)\n",
    "test_y = binary_class.tail(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_counts = count_vect.fit_transform(np.array(train_x))\n",
    "X_train_tfidf= tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "X_test_counts = count_vect.transform(np.array(test_x))\n",
    "X_test_tfidf= tfidf_transformer.fit_transform(X_test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model (Binary Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_model = MultinomialNB().fit(X_train_tfidf,np.array(train_y).astype('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model (Binary Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_labels = clf_model.predict(X_test_tfidf)\n",
    "actualLabels = np.array(test_y).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[104,   0],\n",
       "       [  3,  93]], dtype=int64)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(actualLabels, predict_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      " Classifier Test Score : 0.985\n",
      " f1score : 0.98\n"
     ]
    }
   ],
   "source": [
    "clf_test_score = clf_model.score(X_test_tfidf,actualLabels)\n",
    "print(\"\\n\"+100*\"-\")\n",
    "print(\" Classifier Test Score : \"+str(clf_test_score))\n",
    "\n",
    "precision = round(precision_score(actualLabels, predict_labels,average='macro'),2)\n",
    "recall = round(recall_score(actualLabels, predict_labels,average='macro'),2)\n",
    "f1 = round(f1_score(actualLabels, predict_labels,average='macro'),2)\n",
    "print(\" f1score : \"+str(f1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose what is test and what is train (Multi Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y = util.train_test_multi_class(train,test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model (Multi Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf, X_test_tfidf = util.create_classified_sets(train_x, test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_model = MultinomialNB().fit(X_train_tfidf,np.array(train_y).astype('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model (Multi Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_labels = clf_model.predict(X_test_tfidf)\n",
    "actualLabels = np.array(test_y).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_test_score = clf_model.score(X_test_tfidf,actualLabels)\n",
    "print(\"\\n\"+100*\"-\")\n",
    "print(\" Classifier Test Score : \"+str(clf_test_score))\n",
    "\n",
    "\n",
    "f1 = round(f1_score(actualLabels, predict_labels,average='macro'),2)\n",
    "print(\" f1score : \"+str(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brute Force!!! (Binary Class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initilize Verfication sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 100\n",
    "split = int(total/2)\n",
    "train = int(0.8*total)\n",
    "test = int(0.2*total)\n",
    "\n",
    "threshold = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "count_vect = util.create_vectorizor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_projects = len(df[\"req1Product\"].value_counts()) - 1\n",
    "df_unique = df_dp[\"req1Product\"].unique()\n",
    "df_unique = np.intersect1d(df_unique, df_ip[\"req1Product\"].unique())\n",
    "df_scores = pd.DataFrame(columns = [\"Train Project\", \"Test Project\", \"Prediction Score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Project 1: Bugzilla\n",
      "Training model passed Verfication\n",
      "Model has been Trained\n",
      "Training Project 2: Calendar\n",
      "Training model passed Verfication\n",
      "Model has been Trained\n",
      "Training Project 3: Core\n",
      "Training model passed Verfication\n",
      "Model has been Trained\n",
      "Training Project 4: Firefox\n",
      "Training model passed Verfication\n",
      "Model has been Trained\n",
      "Training Project 5: Firefox Build System\n",
      "Training model passed Verfication\n",
      "Model has been Trained\n",
      "Training Project 6: MailNews Core\n",
      "Training model passed Verfication\n",
      "Model has been Trained\n",
      "Training Project 7: NSPR\n",
      "Sample size too big, please reduce\n",
      "Training Project 8: NSS\n",
      "Training model passed Verfication\n",
      "Model has been Trained\n",
      "Training Project 9: Other Applications\n",
      "Training model passed Verfication\n",
      "Model has been Trained\n",
      "Training Project 10: SeaMonkey\n",
      "Training model passed Verfication\n",
      "Model has been Trained\n",
      "Training Project 11: Testing\n",
      "Sample size too big, please reduce\n",
      "Training Project 12: Thunderbird\n",
      "Training model passed Verfication\n",
      "Model has been Trained\n",
      "Training Project 13: Toolkit\n",
      "Training model passed Verfication\n",
      "Model has been Trained\n",
      "Training Project 14: Webtools\n",
      "Sample size too big, please reduce\n"
     ]
    }
   ],
   "source": [
    "project_num = 0\n",
    "for df_name in df_unique:\n",
    "    ### Training and Verfication Phase ####\n",
    "    project_num = project_num + 1\n",
    "    print(\"Training Project {}: {}\".format(project_num, df_name))\n",
    "    ## get the pairs from both dataset with the same req1product\n",
    "    d_pairs = df_dp[df_dp[\"req1Product\"] == df_name]\n",
    "    i_pairs = df_ip[df_ip[\"req1Product\"] == df_name]\n",
    "    ## if the sample size is too big, let the user know, so that they can change it\n",
    "    if (split > len(d_pairs) or split > len(i_pairs)):\n",
    "        print(\"Sample size too big, please reduce\")\n",
    "        continue\n",
    "    df = d_pairs.head(split)\n",
    "    df = df.append(i_pairs.head(split))\n",
    "    df = df.sample(frac = 1)\n",
    "    train_x, train_y, test_x, test_y = util.train_test_split(df, train, test, \"Binary\")\n",
    "    ## condense the x values for the train and test models\n",
    "    X_train_counts = count_vect.fit_transform(np.array(train_x))\n",
    "    X_train_tfidf= tfidf_transformer.fit_transform(X_train_counts)\n",
    "    X_test_counts = count_vect.transform(np.array(test_x))\n",
    "    X_test_tfidf= tfidf_transformer.fit_transform(X_test_counts)\n",
    "    ## train the model\n",
    "    clf_model = MultinomialNB().fit(X_train_tfidf,np.array(train_y).astype('int'))\n",
    "    predict_labels = clf_model.predict(X_test_tfidf)\n",
    "    actualLabels = np.array(test_y).astype('int')\n",
    "    \n",
    "    precision = round(precision_score(actualLabels, predict_labels,average='macro'),2)\n",
    "    recall = round(recall_score(actualLabels, predict_labels,average='macro'),2)\n",
    "    f1 = round(f1_score(actualLabels, predict_labels,average='macro'),2)\n",
    "    \n",
    "    if (threshold > f1):\n",
    "        print(\"Training model failed Verification\")\n",
    "        continue\n",
    "    \n",
    "    print(\"Training model passed Verfication\")\n",
    "    #### Training and Testing Phase ####\n",
    "    train_x, train_y = util.x_y_split(df, \"Binary\")\n",
    "    X_train_counts = count_vect.fit_transform(np.array(train_x))\n",
    "    X_train_tfidf= tfidf_transformer.fit_transform(X_train_counts)\n",
    "    ## train and fit the model\n",
    "    clf_model = MultinomialNB().fit(X_train_tfidf,np.array(train_y).astype('int'))\n",
    "    \n",
    "    print(\"Model has been Trained\")\n",
    "    train_project = df_name\n",
    "    \n",
    "    for df_name2 in df_unique:\n",
    "        ### find all test projects, see how it works\n",
    "        if df_name == df_name2:\n",
    "            continue\n",
    "        d_pairs2 = df_dp[df_dp[\"req1Product\"] == df_name2]\n",
    "        i_pairs2 = df_ip[df_ip[\"req1Product\"] == df_name2]\n",
    "        if \n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_num = 0\n",
    "## every project will be used as a training model\n",
    "for df_name in df_unique:\n",
    "    project_num = project_num + 1\n",
    "    train = df[df[\"req1Product\"] == df_name]\n",
    "    independent = len(train[train[\"BinaryClass\"] == 0])\n",
    "    dependent = len(train[train[\"BinaryClass\"] == 1])\n",
    "    ## if a data set only has less than 5 pairs of each, don't even bother training\n",
    "    if ((independent < 5) or (dependent < 5)):\n",
    "        print(\"Not using {}\".format(df_name))\n",
    "        continue\n",
    "    ## if a data set's independent pairs are greater than dependent pairs, balance to match dependent pair count\n",
    "    else:\n",
    "        train = util.balance_train(train)\n",
    "    train_x, train_y = util.x_y_split(train)\n",
    "    ## classify and condense the model\n",
    "    X_train_counts = count_vect.fit_transform(np.array(train_x))\n",
    "    X_train_tfidf= tfidf_transformer.fit_transform(X_train_counts)\n",
    "    ## train and fit the model\n",
    "    clf_model = MultinomialNB().fit(X_train_tfidf,np.array(train_y).astype('int'))\n",
    "    ## prompt the user which model is currently being used to test results\n",
    "    print(\"Currently Testing using {} model, this is project number {} out of {}\".format(df_name, project_num, len(df_unique)))\n",
    "    # every training model will test all other models\n",
    "    for df_name2 in df_unique:\n",
    "        if df_name != df_name2:\n",
    "            test = df[df[\"req1Product\"] == df_name2]\n",
    "            test_x, test_y = util.x_y_split(test)\n",
    "            X_test_counts = count_vect.transform(np.array(test_x))\n",
    "            X_test_tfidf= tfidf_transformer.fit_transform(X_test_counts)\n",
    "            predict_labels = clf_model.predict(X_test_tfidf)\n",
    "            actualLabels = np.array(test_y).astype('int')\n",
    "            ## will use f1 scores to see how well the modelsdo\n",
    "            clf_test_score = clf_model.score(X_test_tfidf,actualLabels)\n",
    "            ## add results to the dataframe\n",
    "            result = {\"Train Project\": df_name, \"Test Project\": df_name2, \"Prediction Score\": \"{:.2f}\".format(clf_test_score)}\n",
    "            df_scores = df_scores.append(result, ignore_index = True)\n",
    "\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.to_csv(\"PredictionScores_BinaryClass.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brute Force!!! (MultiClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.DataFrame(columns = [\"Train Project\", \"Test Project\", \"Prediction Score\"])\n",
    "project_num = 0\n",
    "## every project will be used as a training model\n",
    "for df_name in df_unique:\n",
    "    project_num = project_num + 1\n",
    "    train = df[df[\"req1Product\"] == df_name]\n",
    "    independent = len(df[(df[\"req1Product\"] == df_name) & (df[\"BinaryClass\"] == 0)])\n",
    "    dependent = len(df[(df[\"req1Product\"] == df_name) & (df[\"BinaryClass\"] == 1)])\n",
    "    ## if a data set only has less than 5 pairs of each, don't even bother training\n",
    "    if ((independent < 5) or (dependent < 5)):\n",
    "        print(\"Not using {}\".format(df_name))\n",
    "        continue\n",
    "    ## if a data set's independent pairs are greater than dependent pairs, balance to match dependent pair count\n",
    "    else:\n",
    "        train = util.balance_train(train)\n",
    "    train_x, train_y = util.x_y_multiclass_split(train)\n",
    "    ## classify and condense the model\n",
    "    X_train_counts = count_vect.fit_transform(np.array(train_x))\n",
    "    X_train_tfidf= tfidf_transformer.fit_transform(X_train_counts)\n",
    "    ## train and fit the model\n",
    "    clf_model = MultinomialNB().fit(X_train_tfidf,np.array(train_y).astype('int'))\n",
    "    ## prompt the user which model is currently being used to test results\n",
    "    print(\"Currently Testing using {} model, this is project number {}\".format(df_name, project_num))\n",
    "    # every training model will test all other models\n",
    "    for df_name2 in df_unique:\n",
    "        if df_name != df_name2:\n",
    "            test = df[df[\"req1Product\"] == df_name2]\n",
    "            test_x, test_y = util.x_y_multiclass_split(test)\n",
    "            X_test_counts = count_vect.transform(np.array(test_x))\n",
    "            X_test_tfidf= tfidf_transformer.fit_transform(X_test_counts)\n",
    "            predict_labels = clf_model.predict(X_test_tfidf)\n",
    "            actualLabels = np.array(test_y).astype('int')\n",
    "            ## will use f1 scores to see how well the modelsdo\n",
    "            clf_test_score = clf_model.score(X_test_tfidf,actualLabels)\n",
    "            ## add results to the dataframe\n",
    "            result = {\"Train Project\": df_name, \"Test Project\": df_name2, \"Prediction Score\": \"{:.2f}\".format(clf_test_score)}\n",
    "            df_scores = df_scores.append(result, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.to_csv(\"PredictionScores_MultiClass.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
