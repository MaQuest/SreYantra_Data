{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Naive Bayes Classifaction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utilities as util\n",
    "import importlib\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score,train_test_split\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score,confusion_matrix,classification_report\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(util)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establishing global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAIRS = \"ModelPairs.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PAIRS, low_memory = False)\n",
    "df = df.drop(columns = [\"Unnamed: 0\", \"Unnamed: 0.1\"])\n",
    "\n",
    "print(\"The amount of independent pairs is {}\".format(len(df[df[\"BinaryClass\"] == 0])))\n",
    "print(\"The amount of dependent pairs is {}\".format(len(df[df[\"BinaryClass\"] == 1])))\n",
    "\n",
    "if (len(df[df[\"BinaryClass\"] == 0]) == len(df[df[\"BinaryClass\"] == 1])):\n",
    "    print(\"Equally Split data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = [\"req1Id\", \"req1Priority\", \"req1Release\", \"req1Severity\", \"req1Type\", \"req1Ver\", \"req2Id\", \n",
    "                       \"req2Priority\", \"req2Release\", \"req2Severity\", \"req2Type\", \"req2Ver\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the value counts of product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Independent pairs for Core is {}\".format(len(df[(df[\"req1Product\"] == \"Core\") & (df[\"BinaryClass\"] == 0) ])))\n",
    "print(\"dependent pairs for Core is {}\".format(len(df[(df[\"req1Product\"] == \"Core\") & (df[\"BinaryClass\"] == 1) ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Independent pairs for Core is {}\".format(len(df[(df[\"req1Product\"] == \"SeaMonkey\") & (df[\"BinaryClass\"] == 0) ])))\n",
    "print(\"dependent pairs for Core is {}\".format(len(df[(df[\"req1Product\"] == \"SeaMonkey\") & (df[\"BinaryClass\"] == 1) ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating specific dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "df_core = df[df[\"req1Product\"] == \"Core\"]\n",
    "df_firefox = df[df[\"req1Product\"] == \"Firefox\"]\n",
    "df_thunderbird = df[df[\"req1Product\"] == \"Thunderbird\"]\n",
    "df_bugzilla = df[df[\"req1Product\"] == \"Bugzilla\"]\n",
    "df_seamonkey = df[df[\"req1Product\"] == \"SeaMonkey\"]\n",
    "df_least = df[df[\"req1Product\"] == \"Web Compatibility\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = util.balance_train(\"SeaMonkey\", df_seamonkey , True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Independent pairs for Core is {}\".format(len(train_x[(train_x[\"req1Product\"] == \"SeaMonkey\") & (train_x[\"BinaryClass\"] == 0) ])))\n",
    "print(\"dependent pairs for Core is {}\".format(len(train_x[(train_x[\"req1Product\"] == \"SeaMonkey\") & (train_x[\"BinaryClass\"] == 1) ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model for future uses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose what is test and what is train (Binary Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_firefox\n",
    "test = df_core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condense the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = util.x_y_split(train)\n",
    "test_x, test_y = util.x_y_split(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf, X_test_tfidf = util.create_classified_sets(train_x, test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model (Binary Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_model = MultinomialNB().fit(X_train_tfidf,np.array(train_y).astype('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model (Binary Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_labels = clf_model.predict(X_test_tfidf)\n",
    "actualLabels = np.array(test_y).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_test_score = clf_model.score(X_test_tfidf,actualLabels)\n",
    "print(\"\\n\"+100*\"-\")\n",
    "print(\" Classifier Test Score : \"+str(clf_test_score))\n",
    "\n",
    "precision = round(precision_score(actualLabels, predict_labels,average='macro'),2)\n",
    "recall = round(recall_score(actualLabels, predict_labels,average='macro'),2)\n",
    "f1 = round(f1_score(actualLabels, predict_labels,average='macro'),2)\n",
    "print(\" f1score : \"+str(f1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose what is test and what is train (Multi Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y = util.train_test_multi_class(train,test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model (Multi Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf, X_test_tfidf = util.create_classified_sets(train_x, test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_model = MultinomialNB().fit(X_train_tfidf,np.array(train_y).astype('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model (Multi Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_labels = clf_model.predict(X_test_tfidf)\n",
    "actualLabels = np.array(test_y).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_test_score = clf_model.score(X_test_tfidf,actualLabels)\n",
    "print(\"\\n\"+100*\"-\")\n",
    "print(\" Classifier Test Score : \"+str(clf_test_score))\n",
    "\n",
    "\n",
    "f1 = round(f1_score(actualLabels, predict_labels,average='macro'),2)\n",
    "print(\" f1score : \"+str(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brute Force!!! (Binary Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "count_vect = util.create_vectorizor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_projects = len(df[\"req1Product\"].value_counts()) - 1\n",
    "df_unique = df[\"req1Product\"].unique()\n",
    "df_scores = pd.DataFrame(columns = [\"Train Project\", \"Test Project\", \"Prediction Score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_num = 0\n",
    "## every project will be used as a training model\n",
    "for df_name in df_unique:\n",
    "    project_num = project_num + 1\n",
    "    train = df[df[\"req1Product\"] == df_name]\n",
    "    independent = len(df[(df[\"req1Product\"] == df_name) & (df[\"BinaryClass\"] == 0)])\n",
    "    dependent = len(df[(df[\"req1Product\"] == df_name) & (df[\"BinaryClass\"] == 1)])\n",
    "    ## if a data set only has less than 5 pairs of each, don't even bother training\n",
    "    if ((independent == 0) or (dependent == 0)):\n",
    "        print(\"Not using {}\".format(df_name))\n",
    "        continue\n",
    "    ## if a data set's independent pairs are greater than dependent pairs, balance to match dependent pair count\n",
    "    elif (independent > dependent):\n",
    "        train = util.balance_train(df_name, train, True)\n",
    "    ## if a data set's dependent pairs are greater than independent pairs, balance to match independent pair count\n",
    "    elif (dependent > independent):\n",
    "        train = util.balance_train(df_name, train, False)\n",
    "    else: \n",
    "        train = train\n",
    "    train_x, train_y = util.x_y_split(train)\n",
    "    ## classify and condense the model\n",
    "    X_train_counts = count_vect.fit_transform(np.array(train_x))\n",
    "    X_train_tfidf= tfidf_transformer.fit_transform(X_train_counts)\n",
    "    ## train and fit the model\n",
    "    clf_model = MultinomialNB().fit(X_train_tfidf,np.array(train_y).astype('int'))\n",
    "    ## prompt the user which model is currently being used to test results\n",
    "    print(\"Currently Testing using {} model, this is project number {} out of {}\".format(df_name, project_num, len(df_unique)))\n",
    "    # every training model will test all other models\n",
    "    for df_name2 in df_unique:\n",
    "        if df_name != df_name2:\n",
    "            test = df[df[\"req1Product\"] == df_name2]\n",
    "            test_x, test_y = util.x_y_split(test)\n",
    "            X_test_counts = count_vect.transform(np.array(test_x))\n",
    "            X_test_tfidf= tfidf_transformer.fit_transform(X_test_counts)\n",
    "            predict_labels = clf_model.predict(X_test_tfidf)\n",
    "            actualLabels = np.array(test_y).astype('int')\n",
    "            ## will use f1 scores to see how well the modelsdo\n",
    "            clf_test_score = clf_model.score(X_test_tfidf,actualLabels)\n",
    "            ## add results to the dataframe\n",
    "            result = {\"Train Project\": df_name, \"Test Project\": df_name2, \"Prediction Score\": \"{:.2f}\".format(clf_test_score)}\n",
    "            df_scores = df_scores.append(result, ignore_index = True)\n",
    "\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.to_csv(\"PredictionScores_BinaryClass_Retry.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brute Force!!! (MultiClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.DataFrame(columns = [\"Train Project\", \"Test Project\", \"Prediction Score\"])\n",
    "project_num = 0\n",
    "## every project will be used as a training model\n",
    "for df_name in df_unique:\n",
    "    project_num = project_num + 1\n",
    "    train = df[df[\"req1Product\"] == df_name]\n",
    "    independent = len(df[(df[\"req1Product\"] == df_name) & (df[\"BinaryClass\"] == 0)])\n",
    "    dependent = len(df[(df[\"req1Product\"] == df_name) & (df[\"BinaryClass\"] == 1)])\n",
    "    ## if a data set only has less than 5 pairs of each, don't even bother training\n",
    "    if ((independent == 0) or (dependent == 0)):\n",
    "        print(\"Not using {}\".format(df_name))\n",
    "        continue\n",
    "    ## if a data set's independent pairs are greater than dependent pairs, balance to match dependent pair count\n",
    "    elif (independent > dependent):\n",
    "        train = util.balance_train(df_name, train, True)\n",
    "    ## if a data set's dependent pairs are greater than independent pairs, balance to match independent pair count\n",
    "    elif (dependent > independent):\n",
    "        train = util.balance_train(df_name, train, False)\n",
    "    else: \n",
    "        train = train\n",
    "    train_x, train_y = util.x_y_multiclass_split(train)\n",
    "    ## classify and condense the model\n",
    "    X_train_counts = count_vect.fit_transform(np.array(train_x))\n",
    "    X_train_tfidf= tfidf_transformer.fit_transform(X_train_counts)\n",
    "    ## train and fit the model\n",
    "    clf_model = MultinomialNB().fit(X_train_tfidf,np.array(train_y).astype('int'))\n",
    "    ## prompt the user which model is currently being used to test results\n",
    "    print(\"Currently Testing using {} model, this is project number {}\".format(df_name, project_num))\n",
    "    # every training model will test all other models\n",
    "    for df_name2 in df_unique:\n",
    "        if df_name != df_name2:\n",
    "            test = df[df[\"req1Product\"] == df_name2]\n",
    "            test_x, test_y = util.x_y_multiclass_split(test)\n",
    "            X_test_counts = count_vect.transform(np.array(test_x))\n",
    "            X_test_tfidf= tfidf_transformer.fit_transform(X_test_counts)\n",
    "            predict_labels = clf_model.predict(X_test_tfidf)\n",
    "            actualLabels = np.array(test_y).astype('int')\n",
    "            ## will use f1 scores to see how well the modelsdo\n",
    "            clf_test_score = clf_model.score(X_test_tfidf,actualLabels)\n",
    "            ## add results to the dataframe\n",
    "            result = {\"Train Project\": df_name, \"Test Project\": df_name2, \"Prediction Score\": \"{:.2f}\".format(clf_test_score)}\n",
    "            df_scores = df_scores.append(result, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.to_csv(\"PredictionScores_MultiClass_Retry.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
