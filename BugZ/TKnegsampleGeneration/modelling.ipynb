{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Naive Bayes Classifaction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utilities as util\n",
    "import importlib\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score,train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score,confusion_matrix,classification_report\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(util)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establishing global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_pairs = \"Teddy_Data/AllDependentPairs.csv\"\n",
    "I_pairs = \"Teddy_Data/AllIndependentPairs.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dp = pd.read_csv(D_pairs, low_memory = False)\n",
    "df_ip = pd.read_csv(I_pairs, low_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnamed_columns = [\"Unnamed: 0\", \"Unnamed: 0.1\"]\n",
    "df_dp = df_dp.drop(columns = unnamed_columns)\n",
    "df_ip = df_ip.drop(columns = unnamed_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The amount of independent pairs is {}\".format(len(df_dp)))\n",
    "print(\"The amount of dependent pairs is {}\".format(len(df_ip)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model for future uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "count_vect = util.create_vectorizor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1000 for training, 500 for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 1000\n",
    "test_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_dp[df_dp[\"req1Product\"] == \"Core\"].sample(int(train_size/2))\n",
    "df = df.append(df_ip[df_ip[\"req1Product\"] == \"Core\"].sample(int(train_size/2)))\n",
    "# randomize the dataframe\n",
    "df = df.sample(frac = 1)\n",
    "# get the test series'\n",
    "binary_class = np.array(df[\"BinaryClass\"])\n",
    "multi_class = df[\"MultiClass\"]\n",
    "## drop unimportant columns from training\n",
    "train_df = df.drop(columns = ['BinaryClass', 'MultiClass',\"req1Product\",\"req2Product\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split up the train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_counts = count_vect.fit_transform(np.array(train_df))\n",
    "X_train_tfidf= tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "#X_test_counts = count_vect.transform(np.array(test_x))\n",
    "#X_test_tfidf= tfidf_transformer.fit_transform(X_test_counts)\n",
    "scores = cross_val_score(clf_model, X_train_tfidf, binary_class, cv = skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model (Binary Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_model = MultinomialNB().fit(X_train_tfidf,binary_class.astype('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model (Binary Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    test_df = df_dp[df_dp[\"req1Product\"] == \"Firefox\"].sample(int(test_size/2))\n",
    "    test_df = test_df.append(df_ip[df_ip[\"req1Product\"] == \"Firefox\"].sample(int(test_size/2)))\n",
    "    test_df = test_df.sample(frac = 1)\n",
    "    test_binary = np.array(test_df[\"BinaryClass\"])\n",
    "    test_df = test_df.drop(columns = ['BinaryClass', 'MultiClass',\"req1Product\",\"req2Product\"])\n",
    "\n",
    "    X_test_counts = count_vect.transform(np.array(test_df))\n",
    "    X_test_tfidf= tfidf_transformer.fit_transform(X_test_counts)\n",
    "\n",
    "    predict_labels = clf_model.predict(X_test_tfidf)\n",
    "    actualLabels = np.array(test_binary).astype('int')\n",
    "\n",
    "    confusion_matrix(actualLabels, predict_labels)\n",
    "\n",
    "    clf_test_score = clf_model.score(X_test_tfidf,actualLabels)\n",
    "    print(\"\\n\"+100*\"-\")\n",
    "    print(\" Classifier Test Score : \"+str(clf_test_score))\n",
    "\n",
    "    precision = round(precision_score(actualLabels, predict_labels,average='macro'),2)\n",
    "    recall = round(recall_score(actualLabels, predict_labels,average='macro'),2)\n",
    "    f1 = round(f1_score(actualLabels, predict_labels,average='macro'),2)\n",
    "    print(\" f1score : \"+str(f1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose what is test and what is train (Multi Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y = util.train_test_multi_class(train,test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model (Multi Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf, X_test_tfidf = util.create_classified_sets(train_x, test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_model = MultinomialNB().fit(X_train_tfidf,np.array(train_y).astype('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model (Multi Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_labels = clf_model.predict(X_test_tfidf)\n",
    "actualLabels = np.array(test_y).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_test_score = clf_model.score(X_test_tfidf,actualLabels)\n",
    "print(\"\\n\"+100*\"-\")\n",
    "print(\" Classifier Test Score : \"+str(clf_test_score))\n",
    "\n",
    "\n",
    "f1 = round(f1_score(actualLabels, predict_labels,average='macro'),2)\n",
    "print(\" f1score : \"+str(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brute Force!!! (Binary Class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initilize Verfication sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 100\n",
    "split = int(total/2)\n",
    "train = int(0.8*total)\n",
    "test = int(0.2*total)\n",
    "\n",
    "threshold = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "count_vect = util.create_vectorizor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_projects = len(df[\"req1Product\"].value_counts()) - 1\n",
    "df_unique = df_dp[\"req1Product\"].unique()\n",
    "df_unique = np.intersect1d(df_unique, df_ip[\"req1Product\"].unique())\n",
    "df_scores = pd.DataFrame(columns = [\"Train Project\", \"Test Project\", \"Prediction Score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### important global variables for model to use\n",
    "\n",
    "train_size = 1000\n",
    "test_size = 500\n",
    "important_projects = [\"Core\", \"Firefox\", \"Thunderbird\" ,\"Bugzilla\", \"SeaMonkey\"]\n",
    "clf = MultinomialNB()\n",
    "skf = StratifiedKFold(10)\n",
    "\n",
    "new_results = pd.DataFrame(columns = [\"Train Project\", \"Test Project\", \"Average Validation Score (10*10 fold)\",\"Average f1 score (10 Tests)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_num = 0\n",
    "for df_name in important_projects:\n",
    "    ### Training and Verfication Phase ####\n",
    "    project_num = project_num + 1\n",
    "    print(\"Training Project {}: {}\".format(project_num, df_name))\n",
    "    ## get the pairs from both dataset with the same req1product\n",
    "    d_pairs = df_dp[df_dp[\"req1Product\"] == df_name]\n",
    "    i_pairs = df_ip[df_ip[\"req1Product\"] == df_name]\n",
    "    train_df = d_pairs.sample(int(train_size/2))\n",
    "    train_df = train_df.append(i_pairs.sample(int(train_size/2)))\n",
    "    ### randomize the data frame\n",
    "    train_df = train_df.sample(frac = 1)\n",
    "    train_binary_class = np.array(train_df[\"BinaryClass\"])\n",
    "    train_df = train_df.drop(columns = ['BinaryClass', 'MultiClass',\"req1Product\",\"req2Product\"])\n",
    "    ## condense the data\n",
    "    X_train_counts = count_vect.fit_transform(np.array(train_df))\n",
    "    X_train_tfidf= tfidf_transformer.fit_transform(X_train_counts)\n",
    "    ## k-fold validation\n",
    "    scores = cross_val_score(clf_model, X_train_tfidf, train_binary_class, cv = skf)\n",
    "    avgValidation = np.average(scores)\n",
    "    ## checking for a high enough validation score\n",
    "    if (avgValidation < 0.9):\n",
    "        continue\n",
    "    clf_model = MultinomialNB().fit(X_train_tfidf,train_binary_class.astype('int'))\n",
    "    for df_name2 in important_projects:\n",
    "        if df_name == df_name2:\n",
    "            continue\n",
    "        d_pairs2 = df_dp[df_dp[\"req1Product\"] == df_name2]\n",
    "        i_pairs2 = df_ip[df_ip[\"req1Product\"] == df_name2]\n",
    "        ## we want to run this 10 times and take 10 different random samples\n",
    "        f1_scores = []\n",
    "        for i in range(10):\n",
    "            test_df = d_pairs2.sample(int(test_size/2))\n",
    "            test_df = test_df.append(i_pairs2.sample(int(test_size/2)))\n",
    "            test_df = test_df.sample(frac = 1)\n",
    "            test_binary = np.array(test_df[\"BinaryClass\"])\n",
    "            test_df = test_df.drop(columns = ['BinaryClass', 'MultiClass',\"req1Product\",\"req2Product\"])\n",
    "            ## condense the data \n",
    "            X_test_counts = count_vect.transform(np.array(test_df))\n",
    "            X_test_tfidf= tfidf_transformer.fit_transform(X_test_counts)\n",
    "            ## seperate prediction array from actual value array\n",
    "            predict_labels = clf_model.predict(X_test_tfidf)\n",
    "            actualLabels = np.array(test_binary).astype('int')\n",
    "            ## create the confusion matrix\n",
    "            cm = confusion_matrix(actualLabels, predict_labels)\n",
    "            \n",
    "            precision = round(precision_score(actualLabels, predict_labels,average='macro'),2)\n",
    "            recall = round(recall_score(actualLabels, predict_labels,average='macro'),2)\n",
    "            f1 = round(f1_score(actualLabels, predict_labels,average='macro'),2)\n",
    "            f1_scores.append(f1)\n",
    "            \n",
    "        avgf1score = np.average(f1_scores)\n",
    "        result = {\"Train Project\" : df_name,\n",
    "                  \"Test Project\": df_name2,\n",
    "                  \"Average Validation Score (10*10 fold)\" : avgValidation,\n",
    "                  \"Average f1 score (10 Tests)\" : avgf1score}\n",
    "        new_results = new_results.append(result, ignore_index = True)\n",
    "    \n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_results.to_csv(\"New_Results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_num = 0\n",
    "## every project will be used as a training model\n",
    "for df_name in df_unique:\n",
    "    project_num = project_num + 1\n",
    "    train = df[df[\"req1Product\"] == df_name]\n",
    "    independent = len(train[train[\"BinaryClass\"] == 0])\n",
    "    dependent = len(train[train[\"BinaryClass\"] == 1])\n",
    "    ## if a data set only has less than 5 pairs of each, don't even bother training\n",
    "    if ((independent < 5) or (dependent < 5)):\n",
    "        print(\"Not using {}\".format(df_name))\n",
    "        continue\n",
    "    ## if a data set's independent pairs are greater than dependent pairs, balance to match dependent pair count\n",
    "    else:\n",
    "        train = util.balance_train(train)\n",
    "    train_x, train_y = util.x_y_split(train)\n",
    "    ## classify and condense the model\n",
    "    X_train_counts = count_vect.fit_transform(np.array(train_x))\n",
    "    X_train_tfidf= tfidf_transformer.fit_transform(X_train_counts)\n",
    "    ## train and fit the model\n",
    "    clf_model = MultinomialNB().fit(X_train_tfidf,np.array(train_y).astype('int'))\n",
    "    ## prompt the user which model is currently being used to test results\n",
    "    print(\"Currently Testing using {} model, this is project number {} out of {}\".format(df_name, project_num, len(df_unique)))\n",
    "    # every training model will test all other models\n",
    "    for df_name2 in df_unique:\n",
    "        if df_name != df_name2:\n",
    "            test = df[df[\"req1Product\"] == df_name2]\n",
    "            test_x, test_y = util.x_y_split(test)\n",
    "            X_test_counts = count_vect.transform(np.array(test_x))\n",
    "            X_test_tfidf= tfidf_transformer.fit_transform(X_test_counts)\n",
    "            predict_labels = clf_model.predict(X_test_tfidf)\n",
    "            actualLabels = np.array(test_y).astype('int')\n",
    "            ## will use f1 scores to see how well the modelsdo\n",
    "            clf_test_score = clf_model.score(X_test_tfidf,actualLabels)\n",
    "            ## add results to the dataframe\n",
    "            result = {\"Train Project\": df_name, \"Test Project\": df_name2, \"Prediction Score\": \"{:.2f}\".format(clf_test_score)}\n",
    "            df_scores = df_scores.append(result, ignore_index = True)\n",
    "\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.to_csv(\"PredictionScores_BinaryClass.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brute Force!!! (MultiClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.DataFrame(columns = [\"Train Project\", \"Test Project\", \"Prediction Score\"])\n",
    "project_num = 0\n",
    "## every project will be used as a training model\n",
    "for df_name in df_unique:\n",
    "    project_num = project_num + 1\n",
    "    train = df[df[\"req1Product\"] == df_name]\n",
    "    independent = len(df[(df[\"req1Product\"] == df_name) & (df[\"BinaryClass\"] == 0)])\n",
    "    dependent = len(df[(df[\"req1Product\"] == df_name) & (df[\"BinaryClass\"] == 1)])\n",
    "    ## if a data set only has less than 5 pairs of each, don't even bother training\n",
    "    if ((independent < 5) or (dependent < 5)):\n",
    "        print(\"Not using {}\".format(df_name))\n",
    "        continue\n",
    "    ## if a data set's independent pairs are greater than dependent pairs, balance to match dependent pair count\n",
    "    else:\n",
    "        train = util.balance_train(train)\n",
    "    train_x, train_y = util.x_y_multiclass_split(train)\n",
    "    ## classify and condense the model\n",
    "    X_train_counts = count_vect.fit_transform(np.array(train_x))\n",
    "    X_train_tfidf= tfidf_transformer.fit_transform(X_train_counts)\n",
    "    ## train and fit the model\n",
    "    clf_model = MultinomialNB().fit(X_train_tfidf,np.array(train_y).astype('int'))\n",
    "    ## prompt the user which model is currently being used to test results\n",
    "    print(\"Currently Testing using {} model, this is project number {}\".format(df_name, project_num))\n",
    "    # every training model will test all other models\n",
    "    for df_name2 in df_unique:\n",
    "        if df_name != df_name2:\n",
    "            test = df[df[\"req1Product\"] == df_name2]\n",
    "            test_x, test_y = util.x_y_multiclass_split(test)\n",
    "            X_test_counts = count_vect.transform(np.array(test_x))\n",
    "            X_test_tfidf= tfidf_transformer.fit_transform(X_test_counts)\n",
    "            predict_labels = clf_model.predict(X_test_tfidf)\n",
    "            actualLabels = np.array(test_y).astype('int')\n",
    "            ## will use f1 scores to see how well the modelsdo\n",
    "            clf_test_score = clf_model.score(X_test_tfidf,actualLabels)\n",
    "            ## add results to the dataframe\n",
    "            result = {\"Train Project\": df_name, \"Test Project\": df_name2, \"Prediction Score\": \"{:.2f}\".format(clf_test_score)}\n",
    "            df_scores = df_scores.append(result, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.to_csv(\"PredictionScores_MultiClass.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
