{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_results = \"Results/PredictionScores_BinaryClass.csv\"\n",
    "multiclass_results = \"Results/PredictionScores_MultiClass.csv\"\n",
    "cross_project_dependencies = \"Cross-Project-Dependencies.csv\"\n",
    "dependent_pairs = \"Results/AllDependentPairs.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Model Predicitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = pd.read_csv(binary_results)\n",
    "df_multiclass_predictions = pd.read_csv(multiclass_results)\n",
    "df_cross_project = pd.read_csv(cross_project_dependencies)\n",
    "df_pairs = pd.read_csv(\"Teddy_Data/ModelPairs.csv\", low_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions[\"Train Project\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Out Predictions we care about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_projects = [\"Core\", \"Firefox\", \"Thunderbird\", \"Bugzilla\", \"Seamonkey\", \"DevTools\", \"MailNews Core\", \"Toolkit\", \"Testing\", \n",
    "            \"Infrastructure & Operations\", \"NSS\"]\n",
    "\n",
    "train_projects = ['Core', 'MailNews Core', 'SeaMonkey', 'Bugzilla', 'Firefox',\n",
    "       'Other Applications', 'NSS', 'Calendar', 'Thunderbird', 'Toolkit',\n",
    "       'NSPR', 'Testing', 'Firefox Build System', 'Webtools']\n",
    "\n",
    "middle_projects = [\"developer.mozilla.org\", \"Conduit\", \"Developer Documentation\", \"Developer Documentation\", \"Participation Infrastructure\",\n",
    "                  \"Firefox for iOS\", \"NSPR\", \"mozilla.org\", \"Mozilla Foundation Communications\", \"Data Science\", \"Localization Infrastructure and Tools\"]\n",
    "\n",
    "x_ticks = [0,0.2,0.4,0.6,0.8,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = df_predictions.drop(columns = \"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Heat Map (Binary Class Predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select duplicate rows except first occurrence based on all columns\n",
    "df = df_predictions[df_predictions[\"Test Project\"].isin(most_projects)]\n",
    "df = df.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "df = df.pivot(\"Train Project\", \"Test Project\", \"Prediction Score\")\n",
    "ax = sns.heatmap(df,cmap=\"YlGnBu\")\n",
    "ax.set_title(\"Most Prediction Scores (BinaryClass)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Least"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select duplicate rows except first occurrence based on all columns\n",
    "df = df_predictions[df_predictions[\"Test Project\"].isin(least_projects)]\n",
    "df = df.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "df = df.pivot(\"Train Project\", \"Test Project\", \"Prediction Score\")\n",
    "ax = sns.heatmap(df,cmap=\"YlGnBu\")\n",
    "ax.set_title(\"Least Prediction Scores (BinaryClass)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select duplicate rows except first occurrence based on all columns\n",
    "df = df_predictions[df_predictions[\"Test Project\"].isin(middle_projects)]\n",
    "df = df.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "df = df.pivot(\"Train Project\", \"Test Project\", \"Prediction Score\")\n",
    "ax = sns.heatmap(df,cmap=\"YlGnBu\")\n",
    "ax.set_title(\"Median Prediction Scores (BinaryClass)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_predictions.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "df = df.pivot(\"Train Project\", \"Test Project\", \"Prediction Score\")\n",
    "fig, ax = plt.subplots(figsize=(25,15)) \n",
    "ax = sns.heatmap(df,cmap=\"YlGnBu\", ax = ax)\n",
    "ax.set_title(\"Prediction Scores (BinaryClass)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Heat Map (Multi Class Predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_multiclass_predictions[df_multiclass_predictions[\"Test Project\"].isin(most_projects)]\n",
    "df = df.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "df = df.pivot(\"Train Project\", \"Test Project\", \"Prediction Score\")\n",
    "ax = sns.heatmap(df,cmap=\"YlGnBu\")\n",
    "ax.set_title(\"Prediction Scores (MultiClass)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_multiclass_predictions[df_multiclass_predictions[\"Test Project\"].isin(middle_projects)]\n",
    "df = df.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "df = df.pivot(\"Train Project\", \"Test Project\", \"Prediction Score\")\n",
    "ax = sns.heatmap(df,cmap=\"YlGnBu\")\n",
    "ax.set_title(\"Prediction Scores (MultiClass)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_multiclass_predictions.pivot(\"Train Project\", \"Test Project\", \"Prediction Score\")\n",
    "fig, ax = plt.subplots(figsize=(25,15)) \n",
    "ax = sns.heatmap(df,cmap=\"YlGnBu\", ax = ax)\n",
    "ax.set_title(\"Prediction Scores (Multi Class)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at cross-project dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df_cross_project.iloc[0][\"Cross-Project Dependencies\"]\n",
    "df = df_multiclass_predictions[df_multiclass_predictions[\"Train Project\"] == \"Core\"]\n",
    "df = df[df[\"Test Project\"].isin(literal_eval(s))]\n",
    "df = df.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "df = df.pivot(\"Train Project\", \"Test Project\", \"Prediction Score\")\n",
    "fig, ax = plt.subplots(figsize=(25,15)) \n",
    "ax = sns.heatmap(df,cmap=\"YlGnBu\")\n",
    "ax.set_title(\"Prediction Scores (MultiClass)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df_cross_project.iloc[0][\"Cross-Project Dependencies\"]\n",
    "df = df_predictions[df_predictions[\"Train Project\"] == \"Core\"]\n",
    "df = df[df[\"Test Project\"].isin(literal_eval(s))]\n",
    "df = df.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "df = df.pivot(\"Train Project\", \"Test Project\", \"Prediction Score\")\n",
    "fig, ax = plt.subplots(figsize=(25,15)) \n",
    "ax = sns.heatmap(df,cmap=\"YlGnBu\")\n",
    "ax.set_title(\"Prediction Scores (Binary Class)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Firefox Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df_cross_project.iloc[2][\"Cross-Project Dependencies\"]\n",
    "df = df_predictions[df_predictions[\"Train Project\"] == \"Firefox\"]\n",
    "df = df[df[\"Test Project\"].isin(literal_eval(s))]\n",
    "df = df.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "\n",
    "average = np.average(df[\"Prediction Score\"])\n",
    "\n",
    "print(\"Firefox has {} cross-project dependencies of these projects, the average score is {:.2f}\".format(len(df[\"Prediction Score\"]),average))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df_cross_project.iloc[2][\"Cross-Project Dependencies\"]\n",
    "df = df_predictions[df_predictions[\"Train Project\"] == \"Firefox\"]\n",
    "df = df[~df[\"Test Project\"].isin(literal_eval(s))]\n",
    "df = df.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "\n",
    "average = np.average(df[\"Prediction Score\"])\n",
    "\n",
    "print(\"Firefox has {} cross-project dependencies of these projects, the average score is {:.2f}\".format(len(df[\"Prediction Score\"]),average))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df_cross_project.iloc[2][\"Cross-Project Dependencies\"]\n",
    "df = df_predictions[df_predictions[\"Train Project\"] == \"Firefox\"]\n",
    "df = df[df[\"Test Project\"].isin(literal_eval(s))]\n",
    "df = df.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "df = df.pivot(\"Train Project\", \"Test Project\", \"Prediction Score\")\n",
    "fig, ax = plt.subplots(figsize=(25,10)) \n",
    "ax = sns.heatmap(df,cmap=\"YlGnBu\")\n",
    "ax.set_title(\"Firefox Prediction Scores for Cross-Dependent Projects (Binary Class)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df_cross_project.iloc[2][\"Cross-Project Dependencies\"]\n",
    "df = df_predictions[df_predictions[\"Train Project\"] == \"Firefox\"]\n",
    "df = df[~df[\"Test Project\"].isin(literal_eval(s))]\n",
    "df = df.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "df = df.pivot(\"Train Project\", \"Test Project\", \"Prediction Score\")\n",
    "fig, ax = plt.subplots(figsize=(25,10)) \n",
    "ax = sns.heatmap(df,cmap=\"YlGnBu\")\n",
    "ax.set_title(\"Firefox Prediction Scores for Non Cross-Dependent Projects (Binary Class)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mailnews Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df_cross_project.iloc[1][\"Cross-Project Dependencies\"]\n",
    "df = df_predictions[df_predictions[\"Train Project\"] == \"MailNews Core\"]\n",
    "df = df[df[\"Test Project\"].isin(literal_eval(s))]\n",
    "df = df.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "\n",
    "average = np.average(df[\"Prediction Score\"])\n",
    "\n",
    "print(\"Firefox has {} cross-project dependencies of these projects, the average score is {:.2f}\".format(len(df[\"Prediction Score\"]),average))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df_cross_project.iloc[1][\"Cross-Project Dependencies\"]\n",
    "df = df_predictions[df_predictions[\"Train Project\"] == \"MailNews Core\"]\n",
    "df = df[~df[\"Test Project\"].isin(literal_eval(s))]\n",
    "df = df.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "\n",
    "average = np.average(df[\"Prediction Score\"])\n",
    "\n",
    "print(\"Firefox has {} cross-project dependencies of these projects, the average score is {:.2f}\".format(len(df[\"Prediction Score\"]),average))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Prediction Scores from cross-project dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame(columns = [\"Train Project\", \n",
    "                                        \"Total Projects\",\n",
    "                                        \"Cross-Dependent Projects\", \n",
    "                                        \"Binary Class Cross-Dependent Average Prediction Score\", \n",
    "                                        #\"Multi Class Cross-Dependent Average Prediction Score\",\n",
    "                                        #\"Non Cross-Dependent Project Count\",\n",
    "                                        \"Binary Class Non Cross-Dependent Average Prediction Score\"])\n",
    "                                        #\"Multi Class Non Cross-Dependent Average Prediction Score\"])\n",
    "df_predictions.drop_duplicates()\n",
    "\n",
    "for i in range(len(df_cross_project)):\n",
    "    cross_projects = df_cross_project.iloc[i][\"Cross-Project Dependencies\"]\n",
    "    df_name = df_cross_project.iloc[i][\"Project\"]\n",
    "    \n",
    "    if (df_name in df_predictions[\"Train Project\"].unique()):\n",
    "        ## find the predicition for binary class that have cross project dependencies\n",
    "        df = df_predictions[df_predictions[\"Train Project\"] == df_name]\n",
    "        df = df[df[\"Test Project\"].isin(literal_eval(cross_projects))]\n",
    "        df = df.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "\n",
    "        binary_cross_average = np.average(df[\"Prediction Score\"])\n",
    "        cross_count = len(df[\"Prediction Score\"])\n",
    "        \n",
    "        ## find the prediction for multiclass that have cross project dependencies\n",
    "        #df = df_multiclass_predictions[df_multiclass_predictions[\"Train Project\"] == df_name]\n",
    "        #df = df[df[\"Test Project\"].isin(literal_eval(cross_projects))]\n",
    "        #df = df.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "        \n",
    "        #multiclass_cross_average = np.average(df[\"Prediction Score\"])\n",
    "        \n",
    "        ## find the prediction for binary class that don't have cross project dependencies\n",
    "        df = df_predictions[df_predictions[\"Train Project\"] == df_name]\n",
    "        df = df[~df[\"Test Project\"].isin(literal_eval(cross_projects))]\n",
    "        df = df.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "\n",
    "        non_cross_average = np.average(df[\"Prediction Score\"])\n",
    "        non_cross_count = len(df[\"Prediction Score\"])\n",
    "        \n",
    "        ## find the prediction for multi class that don't have cross project dependencies\n",
    "        ##df = df_multiclass_predictions[df_multiclass_predictions[\"Train Project\"] == df_name]\n",
    "        #df = df[~df[\"Test Project\"].isin(literal_eval(cross_projects))]\n",
    "        #df = df.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "        \n",
    "        #multiclass_non_cross_average = np.average(df[\"Prediction Score\"])\n",
    "        \n",
    "        total_project = cross_count + non_cross_count\n",
    "\n",
    "        result = {\"Train Project\" : df_name, \n",
    "                  \"Total Projects\" : total_project,\n",
    "                  \"Cross-Dependent Projects\": cross_count,\n",
    "                  \"Binary Class Cross-Dependent Average Prediction Score\": \"{:.2f}\".format(binary_cross_average), \n",
    "                  #\"Non Cross-Dependent Project Count\": non_cross_count,\n",
    "                  \"Binary Class Non Cross-Dependent Average Prediction Score\": \"{:.2f}\".format(non_cross_average)}\n",
    "\n",
    "        comparison_df = comparison_df.append(result, ignore_index = True)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependent Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
